# -*- coding: utf-8 -*-
"""Copy of Evaluator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jcvueFDfxiEw89bw_aM2ZpUo6DjfCTvS
"""

from google.colab import drive
drive.mount("/content/drive")

#code to ingest the pdf
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

DATA_PATH = 'sourcedocs/'
DB_FAISS_PATH = 'vectorstore/db_faiss'

# Create vector database
def create_vector_db():
    loader = DirectoryLoader(DATA_PATH,
                             glob='*.pdf',
                             loader_cls=PyPDFLoader)

    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,
                                                   chunk_overlap=50)
    texts = text_splitter.split_documents(documents)

    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',
                                       model_kwargs={'device': 'cpu'})

    db = FAISS.from_documents(texts, embeddings)
    db.save_local(DB_FAISS_PATH)

if __name__ == "__main__":
    create_vector_db()

!pip install pypdf langchain torch accelerate bitsandbytes transformers sentence_transformers faiss_gpu -qq -U

!pip install huggingface_hub -qq -U
!pip install ctransformers -qq -U

from huggingface_hub import login
login("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")

"""### **Worked model**"""



!pip install streamlit
!pip install -U langchain-community

# Commented out IPython magic to ensure Python compatibility.
# #with out prompt
# %%writefile qa_interface.py
# import streamlit as st
# from langchain.embeddings import HuggingFaceEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain_community.llms import CTransformers
# from langchain.chains import RetrievalQA
# import google.generativeai as genai
# 
# DB_FAISS_PATH = '/content/drive/MyDrive/vectorstore/db_faiss/'
# 
# # Define functions to load QA model and generate response
# def qa_bot():
#     embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2",
#                                        model_kwargs={'device': 'cpu'})
#     db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)
#     llm = CTransformers(
#         model="TheBloke/Mistral-7B-Instruct-v0.1-GGUF",
#         model_type="llama",
#         max_new_tokens=1000,
#         temperature=0.4
#     )
# 
#     qa_chain = RetrievalQA.from_chain_type(llm=llm,
#                                            chain_type='stuff',
#                                            retriever=db.as_retriever(search_kwargs={'k': 2}),
#                                            return_source_documents=True)
# 
#     return qa_chain
# 
# def generate_response_from_gemini(input_text):
#     genai.configure(api_key="AIzaSyDN64-ZeofcPah4XnVCPzb8eEOHurVU1DE")
#     generation_config = {
#         "temperature": 0.5,
#         "top_p": 1,
#         "top_k": 32,
#         "max_output_tokens": 4096,
#     }
#     safety_settings = [
#         {"category": f"HARM_CATEGORY_{category}", "threshold": "BLOCK_MEDIUM_AND_ABOVE"}
#         for category in ["HARASSMENT", "HATE_SPEECH", "SEXUALLY_EXPLICIT", "DANGEROUS_CONTENT"]
#     ]
#     llm = genai.GenerativeModel(
#         model_name="gemini-pro",
#         generation_config=generation_config,
#         safety_settings=safety_settings,
#     )
#     output = llm.generate_content(input_text)
#     return output.text
# 
# # Streamlit app
# def main():
#     st.title("Question Answering and Response Generation")
# 
#     # User input fields
#     query = st.text_input("Enter the question:")
#     student_answer = st.text_area("Enter the student's answer:")
# 
#     # Button to trigger processing
#     if st.button("Get QA Answer and Generate Response"):
#         # Call QA function
#         qa_result = qa_bot()({'query': query})
#         qa_answer = qa_result['result']
# 
#         # Generate response from Gemini AI
#         input_prompt_template = """
#         You are an answer evaluator. Your job is to give the marks in the form of percentages for the answers given by the students to the actual answer. Before you give the marks to the students take the time to give marks and there are some at most important set of rules to be followed and edge cases to be handled. The input is given in the form of studentAnswer: {studentAnswer}, actualAnswer: {actualAnswer}
#         Set of rules:-
#         1. compare studentAnswer with the actualAnswer.
#         2. make sure that you don't give marks based on the words matched for the studentAnswer and the actualAnswer. Give marks based on comparing the whole meaning of the studentAnswer and the actualAnswer.
#         3. Make sure that you don't give any explanation your work is only to give marks for the studentAnswer in the form of a percentage.
#         4. If there is factual inaccuracy in the studentAnswer then give 0 marks.
#         5. Make sure the output is always in the form of "Percentage: %".
#         6.If the studentAnswer is too short the pecentage is also shoube be less only.The studentAnswer must be contain minimum 100 words
#         There are some edge cases to be handled:-
#         1. If the student repeats the question as the answer give him 0 marks.
#         2. Even if the words in the studentAnswer match with the words in the actualAnswer don't give marks by just considering word matches give marks based on comparing the meaning of the studentAnswer with the actualAnswer."""
# 
#         # Display progress bar
#         with st.spinner("Processing..."):
#             response_text = generate_response_from_gemini(input_prompt_template.format(actualAnswer=qa_answer, studentAnswer=student_answer))
# 
#         # Display results
#         st.subheader("Results:")
#         st.write("Orginal Answer:", qa_answer)
#         st.write("Generated Response percentage:", response_text)
# 
# if __name__ == "__main__":
#     main()
#

# Commented out IPython magic to ensure Python compatibility.
# # with msg and bar
# %%writefile qa_interface.py
# import streamlit as st
# 
# # Import necessary functions and libraries from your code
# from langchain.embeddings import HuggingFaceEmbeddings
# from langchain.vectorstores import FAISS
# from langchain import PromptTemplate
# from langchain.llms import CTransformers
# from langchain.chains import RetrievalQA
# import google.generativeai as genai
# 
# DB_FAISS_PATH = '/content/drive/MyDrive/vectorstore/db_faiss/'
# 
# # Define functions to load QA model and generate response
# def qa_bot():
#     embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2",
#                                        model_kwargs={'device': 'cpu'})
#     db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)
#     llm = CTransformers(
#         #model="TheBloke/Mistral-7B-Instruct-v0.1-GGUF",
#         model="TheBloke/Llama-2-7b-Chat-GGUF",
#         model_file="llama-2-7b-chat.Q8_0.gguf",
#         model_type="llama",
#         max_new_tokens=1000,
#         temperature=0.4
#     )
#     prompt = PromptTemplate(template="""Based on the user question generate an answer with 200 to 250 words. The answer should be meaningful.
# 
#     Context: {context}
#     Question: {question}
# 
#     Only return the helpful answer below and nothing else.
#     Helpful answer:
#     """, input_variables=['context', 'question'])
#     qa_chain = RetrievalQA.from_chain_type(llm=llm,
#                                            chain_type='stuff',
#                                            retriever=db.as_retriever(search_kwargs={'k': 2}),
#                                            return_source_documents=True,
#                                            chain_type_kwargs={'prompt': prompt})
#     return qa_chain
# 
# def generate_response_from_gemini(input_text):
#     genai.configure(api_key="AIzaSyDN64-ZeofcPah4XnVCPzb8eEOHurVU1DE")
#     generation_config = {
#         "temperature": 0.5,
#         "top_p": 1,
#         "top_k": 32,
#         "max_output_tokens": 4096,
#     }
#     safety_settings = [
#         {"category": f"HARM_CATEGORY_{category}", "threshold": "BLOCK_MEDIUM_AND_ABOVE"}
#         for category in ["HARASSMENT", "HATE_SPEECH", "SEXUALLY_EXPLICIT", "DANGEROUS_CONTENT"]
#     ]
#     llm = genai.GenerativeModel(
#         model_name="gemini-pro",
#         generation_config=generation_config,
#         safety_settings=safety_settings,
#     )
#     output = llm.generate_content(input_text)
#     return output.text
# 
# # Streamlit app
# def main():
#     st.title("PSCMR SMART EVALUATOR")
# 
#     # User input fields
#     query = st.text_input("Enter the question:")
#     student_answer = st.text_area("Enter the student's answer:")
# 
#     # Button to trigger processing
#     if st.button("START PROCESS"):
#         # Display message indicating processing is ongoing
#         st.text("Processing...")
# 
#         # Display progress bar while processing
#         progress_bar = st.progress(0)
# 
#         # Call QA function
#         qa_result = qa_bot()({'query': query})
#         qa_answer = qa_result['result']
# 
#         # Increment progress bar to 50% after QA answer is received
#         progress_bar.progress(50)
# 
#         # Generate response from Gemini AI
#         input_prompt_template = """
#         You are an interviewer who interviews the students. Your job is to give the marks in the form of percentages for the answers given by the students to the actual answer. Before you give the marks to the students take the time to give marks and there are some at most important set of rules to be followed and edge cases to be handled. The input is given in the form of studentAnswer: {studentAnswer}, actualAnswer: {actualAnswer}
#         Set of rules:-
#         1. compare studentAnswer with the actualAnswer.
#         2. make sure that you don't give marks based on the words matched for the studentAnswer and the actualAnswer. Give marks based on comparing the whole meaning of the studentAnswer and the actualAnswer.
#         3. Make sure that you don't give any explanation your work is only to give marks for the studentAnswer in the form of a percentage.
#         4. If there is factual inaccuracy in the studentAnswer then give 0 marks.
#         5. Make sure the output is always in the form of "Percentage: %"
#         There are some edge cases to be handled:-
#         1. If the student repeats the question as the answer give him 0 marks.
#         2. Even if the words in the studentAnswer match with the words in the actualAnswer don't give marks by just considering word matches give marks based on comparing the meaning of the studentAnswer with the actualAnswer."""
# 
#         response_text = generate_response_from_gemini(input_prompt_template.format(actualAnswer=qa_answer, studentAnswer=student_answer))
# 
#         # Increment progress bar to 100% after response is generated
#         progress_bar.progress(100)
# 
#         # Display results
#         st.subheader("Results:")
#         st.write("QA Answer:", qa_answer)
#         st.write("Generated Response:", response_text)
# 
# if __name__ == "__main__":
#     main()
#

!wget -q -O - ipv4.icanhazip.com

"""**testing**"""





!streamlit run qa_interface.py & npx localtunnel --port 8501

def calculate_perplexity(model, tokenizer, text):
    """
    Calculate perplexity of a language model on a given text.

    Parameters:
    - model: The language model object (e.g., BertForMaskedLM).
    - tokenizer: The tokenizer for the model.
    - text: The sample text (string) to evaluate the model on.

    Returns:
    - perplexity: The perplexity of the model on the sample text.
    """

    # Tokenize the text
    inputs = tokenizer(text, return_tensors="pt")
    input_ids = inputs.input_ids
    attention_mask = inputs.attention_mask

    # Calculate logits and softmax
    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)
        # Outputs contain logits
        logits = outputs.logits
        # Apply softmax to get probabilities
        softmax = torch.nn.functional.softmax(logits, dim=-1)

    # Check the shape of the softmax tensor
    print(f"Shape of softmax tensor: {softmax.shape}")

    # Calculate log probabilities of each token
    log_probs = 0
    N = len(input_ids[0])  # Number of tokens in the input

    # Iterate through each token
    for i in range(1, N):  # Start from the first actual token (skip CLS)
        token_id = input_ids[0][i]
        try:
            # Obtain probability of the token in the current position
            prob = softmax[0, i, token_id]  # Corrected indexing: use the same index for both softmax and input_ids
            # Calculate log probability
            log_prob = torch.log(prob)
            log_probs += log_prob
        except IndexError as e:
            print(f"IndexError: {e}. Please check the dimensions of the softmax tensor.")
            return None

    # Calculate average log probability
    avg_log_prob = log_probs / N

    # Calculate perplexity
    perplexity = torch.exp(-avg_log_prob).item()

    return perplexity

# Load the BertForMaskedLM model and tokenizer
# Example input text
input_text = "what is AI?"

# Calculate perplexity
perplexity = calculate_perplexity(model, tokenizer, input_text)
print("Perplexity of the model on the sample text:", perplexity)

